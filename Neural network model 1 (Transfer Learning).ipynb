{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8f4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the data\n",
    "train_df = pd.read_parquet('train.parquet')\n",
    "external_data_df = pd.read_csv('external_data.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "external_data_df['date'] = pd.to_datetime(external_data_df['date'])\n",
    "\n",
    "#external data has duplicates therefore we want to drop them\n",
    "external_data_df = external_data_df.drop_duplicates(subset='date')\n",
    "\n",
    "# Resample external_data_df to hourly, setting missing values to NaN\n",
    "external_data_df.set_index('date', inplace=True)\n",
    "external_data_df = external_data_df.resample('H').asfreq()\n",
    "\n",
    "# Interpolate to fill missing hourly data\n",
    "external_data_df.interpolate(method='time', inplace=True)\n",
    "\n",
    "# Reset the index\n",
    "external_data_df.reset_index(inplace=True)\n",
    "\n",
    "# Drop the specified columns with 0 non-null values from both dataframes\n",
    "cols_to_drop = [col for col in external_data_df.columns if external_data_df[col].notnull().sum() == 0]\n",
    "external_data_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = pd.merge(train_df, external_data_df, on='date', how='left')\n",
    "\n",
    "# Load lockdown data\n",
    "#lockdown_df = pd.read_csv('lockdown_data.csv')\n",
    "\n",
    "# Convert 'datetime' column to datetime type in lockdown_df\n",
    "#lockdown_df['datetime'] = pd.to_datetime(lockdown_df['datetime'])\n",
    "\n",
    "# Normalize the 'date' in aligned_df for accurate merging\n",
    "#aligned_df['date'] = aligned_df['date'].dt.normalize()\n",
    "\n",
    "# Merge aligned_df with lockdown_df\n",
    "#merged_df = pd.merge(aligned_df, lockdown_df, left_on='date', right_on='datetime', how='left')\n",
    "\n",
    "# Drop the additional 'datetime' column after merging, if necessary\n",
    "#merged_df.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "# Calculate the median for each column and replace NaN values in merged_df\n",
    "medians = merged_df.median(numeric_only=True)\n",
    "merged_df.fillna(medians, inplace=True)\n",
    "\n",
    "# Drop counter_id and counter technical_id as they don't seem to be very useful\n",
    "#merged_df.drop(['counter_id', 'counter_technical_id', 'counter_installation_date'], axis=1, inplace=True)\n",
    "\n",
    "# List of categorical columns to encode\n",
    "#categorical_columns = ['counter_name', 'site_name'] \n",
    "\n",
    "# Convert categorical columns to string type before hashing\n",
    "#merged_df['counter_name'] = merged_df['counter_name'].astype(str)\n",
    "#merged_df['site_name'] = merged_df['site_name'].astype(str)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(['counter_id', 'counter_technical_id', 'counter_installation_date'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "merged_df.drop(['counter_name', 'site_name'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical columns using feature hashing\n",
    "#hasher = FeatureHasher(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38d02db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_parquet('train.parquet')\n",
    "external_data_df = pd.read_csv('external_data.csv')\n",
    "\n",
    "# Drop columns with 0 non-null values from external_data_df\n",
    "cols_to_drop = [col for col in external_data_df.columns if external_data_df[col].notnull().sum() == 0]\n",
    "external_data_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Convert 'date' columns to datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "external_data_df['date'] = pd.to_datetime(external_data_df['date'])\n",
    "\n",
    "# Merge the data on the 'date' column\n",
    "merged_df = pd.merge(train_df, external_data_df, on='date', how='inner')\n",
    "\n",
    "# Calculate the median for each column and replace NaN values\n",
    "medians = merged_df.median(numeric_only=True)\n",
    "merged_df.fillna(medians, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(['counter_id', 'counter_technical_id', 'counter_installation_date'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "merged_df.drop(['counter_name', 'site_name'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features and target\n",
    "y = merged_df['log_bike_count']\n",
    "X = merged_df.drop('log_bike_count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54b9a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 496827 entries, 0 to 496826\n",
      "Data columns (total 55 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   site_id         496827 non-null  int64         \n",
      " 1   bike_count      496827 non-null  float64       \n",
      " 2   date            496827 non-null  datetime64[ns]\n",
      " 3   coordinates     496827 non-null  category      \n",
      " 4   latitude        496827 non-null  float64       \n",
      " 5   longitude       496827 non-null  float64       \n",
      " 6   log_bike_count  496827 non-null  float64       \n",
      " 7   numer_sta       496827 non-null  float64       \n",
      " 8   pmer            496827 non-null  float64       \n",
      " 9   tend            496827 non-null  float64       \n",
      " 10  cod_tend        496827 non-null  float64       \n",
      " 11  dd              496827 non-null  float64       \n",
      " 12  ff              496827 non-null  float64       \n",
      " 13  t               496827 non-null  float64       \n",
      " 14  td              496827 non-null  float64       \n",
      " 15  u               496827 non-null  float64       \n",
      " 16  vv              496827 non-null  float64       \n",
      " 17  ww              496827 non-null  float64       \n",
      " 18  w1              496827 non-null  float64       \n",
      " 19  w2              496827 non-null  float64       \n",
      " 20  n               496827 non-null  float64       \n",
      " 21  nbas            496827 non-null  float64       \n",
      " 22  hbas            496827 non-null  float64       \n",
      " 23  cl              496827 non-null  float64       \n",
      " 24  cm              496827 non-null  float64       \n",
      " 25  ch              496827 non-null  float64       \n",
      " 26  pres            496827 non-null  float64       \n",
      " 27  tend24          496827 non-null  float64       \n",
      " 28  tn12            496827 non-null  float64       \n",
      " 29  tx12            496827 non-null  float64       \n",
      " 30  tminsol         496827 non-null  float64       \n",
      " 31  raf10           496827 non-null  float64       \n",
      " 32  rafper          496827 non-null  float64       \n",
      " 33  per             496827 non-null  float64       \n",
      " 34  etat_sol        496827 non-null  float64       \n",
      " 35  ht_neige        496827 non-null  float64       \n",
      " 36  ssfrai          496827 non-null  float64       \n",
      " 37  perssfrai       496827 non-null  float64       \n",
      " 38  rr1             496827 non-null  float64       \n",
      " 39  rr3             496827 non-null  float64       \n",
      " 40  rr6             496827 non-null  float64       \n",
      " 41  rr12            496827 non-null  float64       \n",
      " 42  rr24            496827 non-null  float64       \n",
      " 43  nnuage1         496827 non-null  float64       \n",
      " 44  ctype1          496827 non-null  float64       \n",
      " 45  hnuage1         496827 non-null  float64       \n",
      " 46  nnuage2         496827 non-null  float64       \n",
      " 47  ctype2          496827 non-null  float64       \n",
      " 48  hnuage2         496827 non-null  float64       \n",
      " 49  nnuage3         496827 non-null  float64       \n",
      " 50  ctype3          496827 non-null  float64       \n",
      " 51  hnuage3         496827 non-null  float64       \n",
      " 52  nnuage4         496827 non-null  float64       \n",
      " 53  ctype4          496827 non-null  float64       \n",
      " 54  hnuage4         496827 non-null  float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(52), int64(1)\n",
      "memory usage: 209.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b59e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coordinates']\n"
     ]
    }
   ],
   "source": [
    "# Extracting columns which have datatype 'category'\n",
    "categorical_columns = merged_df.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Outputting the categorical variables\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17b58a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.846028,2.375429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.846028,2.375429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.846028,2.375429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.846028,2.375429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.846028,2.375429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coordinates\n",
       "0  48.846028,2.375429\n",
       "1  48.846028,2.375429\n",
       "2  48.846028,2.375429\n",
       "3  48.846028,2.375429\n",
       "4  48.846028,2.375429"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[categorical_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45d87a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'aligned_df' contains the 'coordinates' column in the form of 'latitude,longitude'.\n",
    "\n",
    "# Split 'coordinates' into 'latitude' and 'longitude'\n",
    "merged_df[['latitude', 'longitude']] = merged_df['coordinates'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "#applied log to make the coordinates smaller\n",
    "# Replace 1 with a smaller number if necessary to avoid taking log(0).\n",
    "merged_df['log_latitude'] = np.log(merged_df['latitude'] + 1)\n",
    "merged_df['log_longitude'] = np.log(merged_df['longitude'] + 1)\n",
    "\n",
    "\n",
    "# Initialize the ordinal encoder\n",
    "#compare with featurehasher\n",
    "#encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform 'site_name' and 'counter_name' using ordinal encoding\n",
    "#merged_df['site_name_encoded'] = encoder.fit_transform(merged_df[['site_name']])\n",
    "#merged_df['counter_name_encoded'] = encoder.fit_transform(merged_df[['counter_name']])\n",
    "\n",
    "# Drop the original 'site_name', 'counter_name', and 'coordinates' columns\n",
    "#merged_df.drop(['site_name', 'counter_name', 'coordinates'], axis=1, inplace=True)\n",
    "merged_df.drop(['coordinates'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac48b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 496827 entries, 0 to 496826\n",
      "Data columns (total 56 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   site_id         496827 non-null  int64         \n",
      " 1   bike_count      496827 non-null  float64       \n",
      " 2   date            496827 non-null  datetime64[ns]\n",
      " 3   latitude        496827 non-null  float64       \n",
      " 4   longitude       496827 non-null  float64       \n",
      " 5   log_bike_count  496827 non-null  float64       \n",
      " 6   numer_sta       496827 non-null  float64       \n",
      " 7   pmer            496827 non-null  float64       \n",
      " 8   tend            496827 non-null  float64       \n",
      " 9   cod_tend        496827 non-null  float64       \n",
      " 10  dd              496827 non-null  float64       \n",
      " 11  ff              496827 non-null  float64       \n",
      " 12  t               496827 non-null  float64       \n",
      " 13  td              496827 non-null  float64       \n",
      " 14  u               496827 non-null  float64       \n",
      " 15  vv              496827 non-null  float64       \n",
      " 16  ww              496827 non-null  float64       \n",
      " 17  w1              496827 non-null  float64       \n",
      " 18  w2              496827 non-null  float64       \n",
      " 19  n               496827 non-null  float64       \n",
      " 20  nbas            496827 non-null  float64       \n",
      " 21  hbas            496827 non-null  float64       \n",
      " 22  cl              496827 non-null  float64       \n",
      " 23  cm              496827 non-null  float64       \n",
      " 24  ch              496827 non-null  float64       \n",
      " 25  pres            496827 non-null  float64       \n",
      " 26  tend24          496827 non-null  float64       \n",
      " 27  tn12            496827 non-null  float64       \n",
      " 28  tx12            496827 non-null  float64       \n",
      " 29  tminsol         496827 non-null  float64       \n",
      " 30  raf10           496827 non-null  float64       \n",
      " 31  rafper          496827 non-null  float64       \n",
      " 32  per             496827 non-null  float64       \n",
      " 33  etat_sol        496827 non-null  float64       \n",
      " 34  ht_neige        496827 non-null  float64       \n",
      " 35  ssfrai          496827 non-null  float64       \n",
      " 36  perssfrai       496827 non-null  float64       \n",
      " 37  rr1             496827 non-null  float64       \n",
      " 38  rr3             496827 non-null  float64       \n",
      " 39  rr6             496827 non-null  float64       \n",
      " 40  rr12            496827 non-null  float64       \n",
      " 41  rr24            496827 non-null  float64       \n",
      " 42  nnuage1         496827 non-null  float64       \n",
      " 43  ctype1          496827 non-null  float64       \n",
      " 44  hnuage1         496827 non-null  float64       \n",
      " 45  nnuage2         496827 non-null  float64       \n",
      " 46  ctype2          496827 non-null  float64       \n",
      " 47  hnuage2         496827 non-null  float64       \n",
      " 48  nnuage3         496827 non-null  float64       \n",
      " 49  ctype3          496827 non-null  float64       \n",
      " 50  hnuage3         496827 non-null  float64       \n",
      " 51  nnuage4         496827 non-null  float64       \n",
      " 52  ctype4          496827 non-null  float64       \n",
      " 53  hnuage4         496827 non-null  float64       \n",
      " 54  log_latitude    496827 non-null  float64       \n",
      " 55  log_longitude   496827 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(54), int64(1)\n",
      "memory usage: 216.1 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b6d4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable 'y'\n",
    "y = merged_df['log_bike_count'].copy()\n",
    "\n",
    "# Drop the target variable and any other non-predictor columns to define the features 'X'\n",
    "X = merged_df.drop(['log_bike_count', 'bike_count'], axis=1)  # Assuming 'bike_count' is also not a predictor\n",
    "\n",
    "# Now split the data into training and cross-validation sets\n",
    "# Typically, a simple way to split is to use a certain percentage for training and the rest for validation\n",
    "# For example, using 80% of the data for training and 20% for cross-validation\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(X) * 0.7)\n",
    "\n",
    "# Split the features and the target variable into training and cross-validation sets\n",
    "X_train = X.iloc[:split_index]\n",
    "X_cross_val = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_cross_val = y.iloc[split_index:]\n",
    "# Your data is now split into training and cross-validation sets and is ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65755302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the number of time steps and features\n",
    "num_timesteps = 3  # for example, using 9 hours of data to predict the next count\n",
    "num_features = X_train.shape[1]\n",
    "# Convert datetime column to a numeric feature, e.g., hour of the day\n",
    "\n",
    "# Reset index if your datetime data is in the index of the DataFrame\n",
    "X_train = X_train.reset_index()\n",
    "X_cross_val = X_cross_val.reset_index()\n",
    "\n",
    "# Extract time-related features from the datetime column\n",
    "# Replace 'index' with the actual column name of your datetime data if it's different\n",
    "X_train['hour'] = X_train['date'].dt.hour\n",
    "X_cross_val['hour'] = X_cross_val['date'].dt.hour\n",
    "\n",
    "# Drop the original datetime column\n",
    "X_train = X_train.drop('date', axis=1)\n",
    "X_cross_val = X_cross_val.drop('date', axis=1)\n",
    "\n",
    "# Now you can standardize your features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cross_val_scaled = scaler.transform(X_cross_val)\n",
    "\n",
    "# Function to create sequences of time steps\n",
    "def create_sequences(data, y, time_steps=num_timesteps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        v = data.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Reshape the training and cross-validation data\n",
    "X_train_seq, y_train_seq = create_sequences(pd.DataFrame(X_train_scaled), y_train)\n",
    "X_cross_val_seq, y_cross_val_seq = create_sequences(pd.DataFrame(X_cross_val_scaled), y_cross_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b44a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10868/10868 [==============================] - 74s 6ms/step - loss: 1.5239\n",
      "Epoch 2/50\n",
      "10868/10868 [==============================] - 74s 7ms/step - loss: 1.2228\n",
      "Epoch 3/50\n",
      "10868/10868 [==============================] - 77s 7ms/step - loss: 1.1114\n",
      "Epoch 4/50\n",
      "10868/10868 [==============================] - 78s 7ms/step - loss: 1.0534\n",
      "Epoch 5/50\n",
      "10868/10868 [==============================] - 72s 7ms/step - loss: 1.0122\n",
      "Epoch 6/50\n",
      "10868/10868 [==============================] - 75s 7ms/step - loss: 0.9761\n",
      "Epoch 7/50\n",
      "10868/10868 [==============================] - 72s 7ms/step - loss: 0.9421\n",
      "Epoch 8/50\n",
      "10868/10868 [==============================] - 75s 7ms/step - loss: 0.9129\n",
      "Epoch 9/50\n",
      "10868/10868 [==============================] - 70s 6ms/step - loss: 0.8901\n",
      "Epoch 10/50\n",
      "10868/10868 [==============================] - 70s 6ms/step - loss: 0.8696\n",
      "Epoch 11/50\n",
      "10868/10868 [==============================] - 71s 6ms/step - loss: 0.8515\n",
      "Epoch 12/50\n",
      "10868/10868 [==============================] - 71s 7ms/step - loss: 0.8336\n",
      "Epoch 13/50\n",
      "10868/10868 [==============================] - 72s 7ms/step - loss: 0.8169\n",
      "Epoch 14/50\n",
      " 8657/10868 [======================>.......] - ETA: 14s - loss: 0.7976"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(3, 55)))  # First LSTM layer with 100 neurons\n",
    "model.add(LSTM(50, return_sequences=True))  # Second LSTM layer with 50 neurons, returning sequences\n",
    "model.add(LSTM(30))  # Third LSTM layer with 30 neurons\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "\n",
    "# Compile and fit the model as usual\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Test the model on the cross-validation data\n",
    "y_pred_nn = model.predict(X_cross_val_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd03213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.9996524837671578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_cross_val_seq, y_pred_nn)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b143fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for Linear Regression on Cross-Validation Set: 1.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the cross-validation data\n",
    "y_pred_linear = linear_regressor.predict(X_cross_val_scaled)\n",
    "\n",
    "# Calculate the RMSE for the cross-validation set\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_cross_val, y_pred_linear))\n",
    "print(f\"Root Mean Squared Error (RMSE) for Linear Regression on Cross-Validation Set: {rmse_linear:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a16bcbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: 0.98\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the XGBoost regressor model\n",
    "xgb_regressor = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the cross-validation data\n",
    "y_pred_xgb = xgb_regressor.predict(X_cross_val_scaled)\n",
    "\n",
    "# Calculate the RMSE for the XGBoost model\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_cross_val, y_pred_xgb))\n",
    "print(f\"Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: {rmse_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37eb02f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: 1.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Assuming y_pred_nn and y_pred_xgboost are the prediction arrays from your neural network and XGBoost models\n",
    "# And assuming y_true is the actual values\n",
    "\n",
    "y_pred_xgb_trimmed = y_pred_xgb[:len(y_pred_nn_reshaped)]\n",
    "# Average predictions from both models\n",
    "y_cross_val_trimmed = y_cross_val[:len(y_pred_nn_reshaped)]\n",
    "average_pred = (y_pred_nn_reshaped + y_pred_xgb_trimmed) / 2\n",
    "# Calculate RMSE\n",
    "ARMSE = np.sqrt(mean_squared_error(y_cross_val_trimmed, average_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: {ARMSE:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b14b4e93",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.07 GiB for an array with shape (33071, 33071) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m y_cross_val_trimmed \u001b[38;5;241m=\u001b[39m y_cross_val[(num_timesteps ):]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Average predictions from both models\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m average_pred \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_nn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_xgb_trimmed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate RMSE\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ARMSE \u001b[38;5;241m=\u001b[39m sqrt(mean_squared_error(y_cross_val_trimmed, average_pred))\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.07 GiB for an array with shape (33071, 33071) and data type float32"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_pred_nn and y_pred_xgb are the prediction arrays from your LSTM and XGBoost models\n",
    "\n",
    "# Trim y_pred_xgb to match the length of y_pred_nn (predictions from LSTM)\n",
    "y_pred_xgb_trimmed = y_pred_xgb[(num_timesteps ):]\n",
    "\n",
    "# Ensure y_cross_val is also trimmed to align with the LSTM predictions\n",
    "y_cross_val_trimmed = y_cross_val[(num_timesteps ):]\n",
    "\n",
    "# Average predictions from both models\n",
    "average_pred = (y_pred_nn + y_pred_xgb_trimmed) / 2\n",
    "\n",
    "# Calculate RMSE\n",
    "ARMSE = sqrt(mean_squared_error(y_cross_val_trimmed, average_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {ARMSE:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b27dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 6 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n6 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 1051, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 534, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 954, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 1528, in __init__\n    self._init(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 1587, in _init\n    it.reraise()\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 575, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 556, in _handle_exception\n    return fn()\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 1280, in next\n    input_data(**self.kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 631, in input_data\n    dispatch_proxy_set_data(self.proxy, new, cat_codes, self._allow_host)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 1331, in dispatch_proxy_set_data\n    _check_data_shape(data)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 57, in _check_data_shape\n    raise ValueError(\"Please reshape the input data into 2-dimensional matrix.\")\nValueError: Please reshape the input data into 2-dimensional matrix.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Fit to the data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Best parameters and model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 6 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n6 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 1051, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 534, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\sklearn.py\", line 954, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 1528, in __init__\n    self._init(\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 1587, in _init\n    it.reraise()\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 575, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 556, in _handle_exception\n    return fn()\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 1280, in next\n    input_data(**self.kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\core.py\", line 631, in input_data\n    dispatch_proxy_set_data(self.proxy, new, cat_codes, self._allow_host)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 1331, in dispatch_proxy_set_data\n    _check_data_shape(data)\n  File \"C:\\Users\\Hoecine\\New folder (2)\\lib\\site-packages\\xgboost\\data.py\", line 57, in _check_data_shape\n    raise ValueError(\"Please reshape the input data into 2-dimensional matrix.\")\nValueError: Please reshape the input data into 2-dimensional matrix.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [12,10],\n",
    "    'learning_rate': [0.02],\n",
    "    'n_estimators': [1000],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.7]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bb996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2d2d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9485270591439028\n",
      "MAE: 0.6099876373993324\n",
      "R^2: 0.5713355561776677\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.02, 'max_depth': 10, 'n_estimators': 1000, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = best_model.predict(X_cross_val)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = mean_squared_error(y_cross_val, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_cross_val, y_pred)\n",
    "r2 = r2_score(y_cross_val, y_pred)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R^2: {r2}\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13e36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
