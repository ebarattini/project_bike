{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a23590b",
   "metadata": {},
   "source": [
    "# MAP 536 - Python for Data Science - Predicting Cyclist Traffic in Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d9a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd3b93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 496827 entries, 0 to 496826\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   bike_count         496827 non-null  float64 \n",
      " 1   coordinates        496827 non-null  category\n",
      " 2   latitude           496827 non-null  float64 \n",
      " 3   longitude          496827 non-null  float64 \n",
      " 4   log_bike_count     496827 non-null  float64 \n",
      " 5   temp               496827 non-null  float64 \n",
      " 6   feelslike          496827 non-null  float64 \n",
      " 7   humidity           496827 non-null  float64 \n",
      " 8   precip             496827 non-null  float64 \n",
      " 9   snow               496661 non-null  float64 \n",
      " 10  snowdepth          496661 non-null  float64 \n",
      " 11  windgust           489727 non-null  float64 \n",
      " 12  windspeed          496827 non-null  float64 \n",
      " 13  cloudcover         496827 non-null  float64 \n",
      " 14  visibility         496661 non-null  float64 \n",
      " 15  is_school_holiday  496827 non-null  int64   \n",
      " 16  is_strike          496827 non-null  int64   \n",
      " 17  full_lockdown      496827 non-null  int64   \n",
      " 18  partial_lockdown   496827 non-null  int64   \n",
      " 19  school_closures    496827 non-null  int64   \n",
      " 20  business_closures  496827 non-null  int64   \n",
      " 21  hour               496827 non-null  int64   \n",
      " 22  year               496827 non-null  int64   \n",
      " 23  month              496827 non-null  int64   \n",
      " 24  day                496827 non-null  int64   \n",
      " 25  weekday            496827 non-null  int64   \n",
      "dtypes: category(1), float64(14), int64(11)\n",
      "memory usage: 99.0 MB\n"
     ]
    }
   ],
   "source": [
    "## + Weather data\n",
    "# Load training and testing datasets & remove unnecessary cols\n",
    "train_data = pd.read_parquet(\"train.parquet\")\n",
    "test_data = pd.read_parquet(\"test.parquet\")\n",
    "train_data.drop(columns=['counter_name', 'site_name','counter_id', 'counter_installation_date', 'counter_technical_id', 'site_id'], inplace=True)\n",
    "test_data.drop(columns=['counter_name', 'site_name','counter_id', 'counter_installation_date', 'counter_technical_id', 'site_id'], inplace=True)\n",
    "\n",
    "#Load weather dataset and remove irrelevant columns\n",
    "weather_data = pd.read_csv(\"hourly-weather-data.csv\")\n",
    "weather_data = weather_data.drop(columns=['name', 'dew', 'precipprob', 'preciptype','uvindex','icon','stations', 'sealevelpressure', 'winddir', 'conditions', 'sealevelpressure', 'severerisk', 'solarradiation', 'solarenergy'])\n",
    "\n",
    "# convert to datetime to merge them properly\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n",
    "\n",
    "#merge with weather data\n",
    "merged_train_data = pd.merge(train_data, weather_data, left_on='date', right_on='datetime', how='inner')\n",
    "merged_test_data = pd.merge(test_data, weather_data, left_on='date', right_on='datetime', how='inner')\n",
    "merged_train_data = merged_train_data.drop(columns=['datetime'])\n",
    "merged_test_data = merged_test_data.drop(columns=['datetime'])\n",
    "\n",
    "\n",
    "## School Holidays + Weather data\n",
    "# import the holiday dataset\n",
    "schoolholiday_data = pd.read_csv(\"fr-calendrier.csv\", delimiter=';')\n",
    "schoolholiday_data = schoolholiday_data[schoolholiday_data['zones'] == 'Zone C'] # Zone C is Paris\n",
    "schoolholiday_data = schoolholiday_data.drop(columns=['description','location','annee_scolaire', 'zones'])\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "schoolholiday_data['start_date'] = pd.to_datetime(schoolholiday_data['start_date'], utc=True).dt.date\n",
    "schoolholiday_data['end_date'] = pd.to_datetime(schoolholiday_data['end_date'],utc=True).dt.date\n",
    "\n",
    "# Generate a set of unique dates for each range in a row\n",
    "unique_dates = set()\n",
    "for _, row in schoolholiday_data.iterrows():\n",
    "    unique_dates.update(pd.date_range(start=row['start_date'], end=row['end_date']))\n",
    "\n",
    "# Convert the set back to a list and create a DataFrame\n",
    "unique_dates_list = sorted(list(unique_dates)) \n",
    "schoolholiday_data = pd.DataFrame({'Date': unique_dates_list})\n",
    "\n",
    "# merge with rest of the data\n",
    "merged_train = pd.merge(merged_train_data, schoolholiday_data, left_on='date', right_on='Date', how='left')\n",
    "merged_train['Date'] = merged_train['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_train.rename(columns={'Date': 'is_school_holiday'}, inplace=True)\n",
    "\n",
    "merged_test = pd.merge(merged_test_data, schoolholiday_data, left_on='date', right_on='Date', how='left')\n",
    "merged_test['Date'] = merged_test['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_test.rename(columns={'Date': 'is_school_holiday'}, inplace=True)\n",
    "\n",
    "\n",
    "## School Holidays + Weather data + strike data\n",
    "# import the strike dataset\n",
    "from datetime import datetime  # Import the datetime class from the datetime module\n",
    "\n",
    "# strike dates for public transport in Paris, retrieved from: https://www.cestlagreve.fr/calendrier/?lieu=74&secteur=14&mois=1&annee=2022\n",
    "strike_dates = {'Date': [datetime(2020, 9, 17), datetime(2020, 12, 14), datetime(2020, 12, 16),\n",
    "                        datetime(2021, 1, 21), datetime(2021, 2, 4), datetime(2021, 2, 15),\n",
    "                        datetime(2021, 5, 21), datetime(2021, 6, 1), datetime(2021, 10, 5),\n",
    "                        datetime(2021, 11, 17)]}\n",
    "\n",
    "strike_data = pd.DataFrame(strike_dates)\n",
    "strike_data\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "strike_data['Date'] = pd.to_datetime(strike_data['Date'])\n",
    "\n",
    "\n",
    "# merge with rest of the data\n",
    "merged_train = pd.merge(merged_train, strike_data, left_on='date', right_on='Date', how='left')\n",
    "merged_train['Date'] = merged_train['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_train.rename(columns={'Date': 'is_strike'}, inplace=True)\n",
    "\n",
    "merged_test = pd.merge(merged_test, strike_data, left_on='date', right_on='Date', how='left')\n",
    "merged_test['Date'] = merged_test['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_test.rename(columns={'Date': 'is_strike'}, inplace=True)\n",
    "merged_test.head()\n",
    "\n",
    "\n",
    "## School Holidays + Weather data + Strike data + Lockdown data\n",
    "lockdown_data = pd.read_csv(\"lockdown_data.csv\")\n",
    "lockdown_data['datetime'] = pd.to_datetime(lockdown_data['datetime'])\n",
    "merged_train['date'] = pd.to_datetime(merged_train['date'])\n",
    "merged_test['date'] = pd.to_datetime(merged_test['date'])\n",
    "\n",
    "merged_train = pd.merge(merged_train, lockdown_data, left_on='date', right_on='datetime', how='left')\n",
    "merged_train = merged_train.drop(columns=['datetime'])\n",
    "merged_test = pd.merge(merged_test, lockdown_data, left_on='date', right_on='datetime', how='left')\n",
    "merged_test = merged_test.drop(columns=['datetime'])\n",
    "\n",
    "# Extract the hour features from the datetime column\n",
    "merged_train['hour'] = merged_train['date'].dt.hour\n",
    "\n",
    "## Encode the dates\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "merged_train = _encode_dates(merged_train)\n",
    "merged_test = _encode_dates(merged_test)\n",
    "\n",
    "# define x and y\n",
    "X_merged_train = merged_train.drop(columns=['bike_count', 'log_bike_count'])\n",
    "Y_merged_train = merged_train['log_bike_count']\n",
    "\n",
    "X_merged_test = merged_test.drop(columns=['bike_count', 'log_bike_count'])\n",
    "Y_merged_test = merged_test['log_bike_count']\n",
    "\n",
    "merged_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3064b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 489727 entries, 0 to 496826\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   bike_count         489727 non-null  float64 \n",
      " 1   coordinates        489727 non-null  category\n",
      " 2   latitude           489727 non-null  float64 \n",
      " 3   longitude          489727 non-null  float64 \n",
      " 4   log_bike_count     489727 non-null  float64 \n",
      " 5   temp               489727 non-null  float64 \n",
      " 6   feelslike          489727 non-null  float64 \n",
      " 7   humidity           489727 non-null  float64 \n",
      " 8   precip             489727 non-null  float64 \n",
      " 9   snow               489727 non-null  float64 \n",
      " 10  snowdepth          489727 non-null  float64 \n",
      " 11  windgust           489727 non-null  float64 \n",
      " 12  windspeed          489727 non-null  float64 \n",
      " 13  cloudcover         489727 non-null  float64 \n",
      " 14  visibility         489727 non-null  float64 \n",
      " 15  is_school_holiday  489727 non-null  int64   \n",
      " 16  is_strike          489727 non-null  int64   \n",
      " 17  full_lockdown      489727 non-null  int64   \n",
      " 18  partial_lockdown   489727 non-null  int64   \n",
      " 19  school_closures    489727 non-null  int64   \n",
      " 20  business_closures  489727 non-null  int64   \n",
      " 21  hour               489727 non-null  int64   \n",
      " 22  year               489727 non-null  int64   \n",
      " 23  month              489727 non-null  int64   \n",
      " 24  day                489727 non-null  int64   \n",
      " 25  weekday            489727 non-null  int64   \n",
      "dtypes: category(1), float64(14), int64(11)\n",
      "memory usage: 97.6 MB\n"
     ]
    }
   ],
   "source": [
    "merged_train=merged_train.dropna()\n",
    "merged_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "289be5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define your features sets for XGBoost and LSTM\n",
    "features_xgb = ['latitude', 'longitude', 'temp', 'feelslike', 'humidity', \n",
    "                'precip', 'snow', 'snowdepth', 'windgust', 'windspeed', 'cloudcover', \n",
    "                'visibility', 'is_school_holiday', 'is_strike', 'full_lockdown', \n",
    "                'partial_lockdown', 'school_closures', 'business_closures', \n",
    "                'hour', 'year', 'month', 'day', 'weekday']\n",
    "\n",
    "features_lstm = ['temp', 'feelslike', 'humidity', 'precip', 'snow', 'snowdepth', \n",
    "                 'windgust', 'windspeed', 'cloudcover', 'visibility', 'is_school_holiday', \n",
    "                 'is_strike', 'full_lockdown', 'partial_lockdown', 'school_closures', \n",
    "                 'business_closures', 'hour', 'year', 'month', 'day', 'weekday']\n",
    "\n",
    "# Target variable\n",
    "target = 'log_bike_count'\n",
    "\n",
    "# Splitting the data for XGBoost\n",
    "X_xgb = merged_train[features_xgb]\n",
    "y_xgb = merged_train[target]\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the data for LSTM - first we scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_lstm = scaler.fit_transform(merged_train[features_lstm])\n",
    "y_lstm = merged_train[target].values\n",
    "\n",
    "# Splitting the scaled data\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_train_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "\n",
    "num_timesteps = 24\n",
    "\n",
    "# Function to create sequences of time steps\n",
    "def create_sequences(data, y, time_steps=num_timesteps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        v = data[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Convert the data into sequences for LSTM\n",
    "X_train_lstm_seq, y_train_lstm_seq = create_sequences(X_train_lstm, y_train_lstm)\n",
    "X_test_lstm_seq, y_test_lstm_seq = create_sequences(X_test_lstm, y_test_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76c24c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: 0.55\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the XGBoost regressor model\n",
    "xgb_regressor = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_regressor.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# Predict on the cross-validation data\n",
    "y_pred_xgb = xgb_regressor.predict(X_test_xgb)\n",
    "\n",
    "# Calculate the RMSE for the XGBoost model\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))\n",
    "print(f\"Root Mean Squared Error (RMSE) for XGBoost on Cross-Validation Set: {rmse_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fec08ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12243/12243 [==============================] - 198s 16ms/step - loss: 2.7670\n",
      "Epoch 2/50\n",
      "12243/12243 [==============================] - 185s 15ms/step - loss: 2.7560\n",
      "Epoch 3/50\n",
      "12243/12243 [==============================] - 175s 14ms/step - loss: 2.7549\n",
      "Epoch 4/50\n",
      "12243/12243 [==============================] - 176s 14ms/step - loss: 2.7539\n",
      "Epoch 5/50\n",
      " 8138/12243 [==================>...........] - ETA: 1:04 - loss: 2.7606"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_lstm_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_lstm_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m y_pred_lstm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_lstm_seq, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\New folder (2)\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define the number of time steps and features\n",
    "num_timesteps = 24   # Using 24 hours of data to predict the next hour\n",
    "\n",
    "num_features = X_train_lstm_seq.shape[2]\n",
    "\n",
    "# Now you can define your model\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, return_sequences=True, input_shape=(num_timesteps, num_features)))\n",
    "model.add(LSTM(30))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train_lstm_seq, y_train_lstm_seq, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_lstm = model.predict(X_test_lstm_seq, verbose=1)\n",
    "\n",
    "# Calculate RMSE for LSTM predictions\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm_seq, y_pred_lstm.flatten()))\n",
    "print(f\"RMSE for LSTM: {rmse_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('lstm_model_weights_01_12_2023.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b94865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with LSTM\n",
    "y_pred_lstm = model.predict(test_generator).flatten()\n",
    "\n",
    "# Align the true target values with LSTM predictions\n",
    "y_true_lstm = y_test[num_timesteps:]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_true_lstm, y_pred_lstm))\n",
    "print(f\"RMSE for LSTM: {rmse_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with LSTM\n",
    "\n",
    "y_pred_lstm = model.predict(X_lstm).flatten()\n",
    "\n",
    "# Align the true target values with LSTM predictions\n",
    "y_true_lstm = y_test[num_timesteps:]\n",
    "# Calculate RMSE\n",
    "\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_true_lstm, y_pred_lstm))\n",
    "print(f\"RMSE for LSTM: {rmse_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_pred_xgb is your XGBoost predictions\n",
    "y_pred_xgb_aligned = y_pred_xgb[num_timesteps:]\n",
    "\n",
    "# Ensure the shapes are the same\n",
    "assert len(y_pred_lstm) == len(y_pred_xgb_aligned)\n",
    "\n",
    "# Now you can average them\n",
    "y_pred_avg = (y_pred_lstm + y_pred_xgb_aligned) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to match the predictions after the averaging process\n",
    "y_true_aligned = y_test_seq[num_timesteps:]\n",
    "\n",
    "# Ensure the shapes are the same for true values and averaged predictions\n",
    "assert len(y_pred_avg) == len(y_true_aligned)\n",
    "\n",
    "# Calculate RMSE for the averaged predictions\n",
    "rmse_avg = np.sqrt(mean_squared_error(y_true_aligned, y_pred_avg.flatten()))\n",
    "print(f\"RMSE for averaged predictions: {rmse_avg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
