{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a23590b",
   "metadata": {},
   "source": [
    "# MAP 536 - Python for Data Science - Predicting Cyclist Traffic in Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d9a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3b93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## + Weather data\n",
    "# Load training and testing datasets & remove unnecessary cols\n",
    "train_data = pd.read_parquet(\"train.parquet\")\n",
    "test_data = pd.read_parquet(\"test.parquet\")\n",
    "final_test_data =pd.read_parquet(\"final_test.parquet\")\n",
    "\n",
    "#combining test data + train data and reassigning to use our old variables in order to change our code minimally\n",
    "train_data = combined_train_test = pd.concat([train_data, test_data])\n",
    "test_data = final_test_data\n",
    "\n",
    "train_data.drop(columns=['counter_name', 'site_name','counter_id', 'counter_installation_date', 'counter_technical_id', 'site_id'], inplace=True)\n",
    "test_data.drop(columns=['counter_name', 'site_name','counter_id', 'counter_installation_date', 'counter_technical_id', 'site_id'], inplace=True)\n",
    "\n",
    "#Load weather dataset and remove irrelevant columns\n",
    "weather_data = pd.read_csv(\"hourly-weather-data.csv\")\n",
    "weather_data = weather_data.drop(columns=['name', 'dew', 'precipprob', 'preciptype','uvindex','icon','stations', 'sealevelpressure', 'winddir', 'conditions', 'sealevelpressure', 'severerisk', 'solarradiation', 'solarenergy'])\n",
    "\n",
    "# convert to datetime to merge them properly\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n",
    "\n",
    "#merge with weather data\n",
    "merged_train_data = pd.merge(train_data, weather_data, left_on='date', right_on='datetime', how='inner')\n",
    "merged_test_data = pd.merge(test_data, weather_data, left_on='date', right_on='datetime', how='inner')\n",
    "merged_train_data = merged_train_data.drop(columns=['datetime'])\n",
    "merged_test_data = merged_test_data.drop(columns=['datetime'])\n",
    "\n",
    "\n",
    "## School Holidays + Weather data\n",
    "# import the holiday dataset\n",
    "schoolholiday_data = pd.read_csv(\"fr-calendrier.csv\", delimiter=';')\n",
    "schoolholiday_data = schoolholiday_data[schoolholiday_data['zones'] == 'Zone C'] # Zone C is Paris\n",
    "schoolholiday_data = schoolholiday_data.drop(columns=['description','location','annee_scolaire', 'zones'])\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "schoolholiday_data['start_date'] = pd.to_datetime(schoolholiday_data['start_date'], utc=True).dt.date\n",
    "schoolholiday_data['end_date'] = pd.to_datetime(schoolholiday_data['end_date'],utc=True).dt.date\n",
    "\n",
    "# Generate a set of unique dates for each range in a row\n",
    "unique_dates = set()\n",
    "for _, row in schoolholiday_data.iterrows():\n",
    "    unique_dates.update(pd.date_range(start=row['start_date'], end=row['end_date']))\n",
    "\n",
    "# Convert the set back to a list and create a DataFrame\n",
    "unique_dates_list = sorted(list(unique_dates)) \n",
    "schoolholiday_data = pd.DataFrame({'Date': unique_dates_list})\n",
    "\n",
    "# merge with rest of the data\n",
    "merged_train = pd.merge(merged_train_data, schoolholiday_data, left_on='date', right_on='Date', how='left')\n",
    "merged_train['Date'] = merged_train['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_train.rename(columns={'Date': 'is_school_holiday'}, inplace=True)\n",
    "\n",
    "merged_test = pd.merge(merged_test_data, schoolholiday_data, left_on='date', right_on='Date', how='left')\n",
    "merged_test['Date'] = merged_test['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_test.rename(columns={'Date': 'is_school_holiday'}, inplace=True)\n",
    "\n",
    "\n",
    "## School Holidays + Weather data + strike data\n",
    "# import the strike dataset\n",
    "from datetime import datetime  # Import the datetime class from the datetime module\n",
    "\n",
    "# strike dates for public transport in Paris, retrieved from: https://www.cestlagreve.fr/calendrier/?lieu=74&secteur=14&mois=1&annee=2022\n",
    "strike_dates = {'Date': [datetime(2020, 9, 17), datetime(2020, 12, 14), datetime(2020, 12, 16),\n",
    "                        datetime(2021, 1, 21), datetime(2021, 2, 4), datetime(2021, 2, 15),\n",
    "                        datetime(2021, 5, 21), datetime(2021, 6, 1), datetime(2021, 10, 5),\n",
    "                        datetime(2021, 11, 17)]}\n",
    "\n",
    "strike_data = pd.DataFrame(strike_dates)\n",
    "strike_data\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "strike_data['Date'] = pd.to_datetime(strike_data['Date'])\n",
    "\n",
    "\n",
    "# merge with rest of the data\n",
    "merged_train = pd.merge(merged_train, strike_data, left_on='date', right_on='Date', how='left')\n",
    "merged_train['Date'] = merged_train['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_train.rename(columns={'Date': 'is_strike'}, inplace=True)\n",
    "\n",
    "merged_test = pd.merge(merged_test, strike_data, left_on='date', right_on='Date', how='left')\n",
    "merged_test['Date'] = merged_test['Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "merged_test.rename(columns={'Date': 'is_strike'}, inplace=True)\n",
    "merged_test.head()\n",
    "\n",
    "\n",
    "## School Holidays + Weather data + Strike data + Lockdown data\n",
    "lockdown_data = pd.read_csv(\"lockdown-data.csv\")\n",
    "lockdown_data['datetime'] = pd.to_datetime(lockdown_data['datetime'])\n",
    "merged_train['date'] = pd.to_datetime(merged_train['date'])\n",
    "merged_test['date'] = pd.to_datetime(merged_test['date'])\n",
    "\n",
    "merged_train = pd.merge(merged_train, lockdown_data, left_on='date', right_on='datetime', how='left')\n",
    "merged_train = merged_train.drop(columns=['datetime'])\n",
    "merged_test = pd.merge(merged_test, lockdown_data, left_on='date', right_on='datetime', how='left')\n",
    "merged_test = merged_test.drop(columns=['datetime'])\n",
    "\n",
    "# Extract the hour features from the datetime column\n",
    "merged_train['hour'] = merged_train['date'].dt.hour\n",
    "merged_test['hour'] = merged_test['date'].dt.hour\n",
    "\n",
    "## Encode the dates\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "merged_train.drop(columns=['coordinates'], inplace=True)\n",
    "\n",
    "merged_train = _encode_dates(merged_train)\n",
    "merged_test = _encode_dates(merged_test)\n",
    "\n",
    "# define x and y\n",
    "X_merged_train = merged_train.drop(columns=['bike_count', 'log_bike_count'])\n",
    "Y_merged_train = merged_train['log_bike_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3064b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 531335 entries, 0 to 538434\n",
      "Data columns (total 25 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   bike_count         531335 non-null  float64\n",
      " 1   latitude           531335 non-null  float64\n",
      " 2   longitude          531335 non-null  float64\n",
      " 3   log_bike_count     531335 non-null  float64\n",
      " 4   temp               531335 non-null  float64\n",
      " 5   feelslike          531335 non-null  float64\n",
      " 6   humidity           531335 non-null  float64\n",
      " 7   precip             531335 non-null  float64\n",
      " 8   snow               531335 non-null  float64\n",
      " 9   snowdepth          531335 non-null  float64\n",
      " 10  windgust           531335 non-null  float64\n",
      " 11  windspeed          531335 non-null  float64\n",
      " 12  cloudcover         531335 non-null  float64\n",
      " 13  visibility         531335 non-null  float64\n",
      " 14  is_school_holiday  531335 non-null  int64  \n",
      " 15  is_strike          531335 non-null  int64  \n",
      " 16  full_lockdown      531335 non-null  int64  \n",
      " 17  partial_lockdown   531335 non-null  int64  \n",
      " 18  school_closures    531335 non-null  int64  \n",
      " 19  business_closures  531335 non-null  int64  \n",
      " 20  hour               531335 non-null  int64  \n",
      " 21  year               531335 non-null  int64  \n",
      " 22  month              531335 non-null  int64  \n",
      " 23  day                531335 non-null  int64  \n",
      " 24  weekday            531335 non-null  int64  \n",
      "dtypes: float64(14), int64(11)\n",
      "memory usage: 105.4 MB\n"
     ]
    }
   ],
   "source": [
    "merged_train=merged_train.dropna()\n",
    "\n",
    "# Drop the 'coordinates' column from the merged_train DataFrame\n",
    "\n",
    "merged_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2204/2805 [======================>.......] - ETA: 3s - loss: nan"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming your preprocessed data is in X_merged_train and Y_merged_train\n",
    "# First, ensure no NaN values in the target\n",
    "Y_merged_train = Y_merged_train.fillna(method='ffill')\n",
    "\n",
    "# Normalize features\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_merged_train.values)\n",
    "\n",
    "# Normalize target variable (if it's not already normalized)\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y_merged_train.values.reshape(-1, 1))\n",
    "\n",
    "# Reshape data for LSTM: [samples, time steps, features]\n",
    "X_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Convert Y to numpy array\n",
    "Y_reshaped = np.reshape(Y_scaled, (Y_scaled.shape[0], 1))\n",
    "\n",
    "# LSTM Model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(1, X_reshaped.shape[2])))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=0.001, clipvalue=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Cross Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train, X_test = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    Y_train, Y_test = Y_reshaped[train_index], Y_reshaped[test_index]\n",
    "\n",
    "    model = create_model()\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "    # Fit model with validation split for early stopping\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "# Example Prediction\n",
    "# Predicting for the last 24 hours in the dataset\n",
    "last_24_hours = X_reshaped[-24:]\n",
    "predicted_bike_count_scaled = model.predict(last_24_hours)\n",
    "\n",
    "# Invert scaling for predicted values\n",
    "predicted_bike_count = scaler_Y.inverse_transform(predicted_bike_count_scaled)\n",
    "\n",
    "# Display the predictions\n",
    "print(predicted_bike_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
