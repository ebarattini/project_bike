{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0554c464",
   "metadata": {},
   "source": [
    "# MAP 536 - Python for Data Science - Predicting Cyclist Traffic in Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d475c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(data, base_filename):\n",
    "    \"\"\"\n",
    "    Converts the input data to a DataFrame (if not already) and saves it to a CSV file with the \n",
    "    current date and time appended to the filename. Automatically prints the filename of the saved CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    data: Data to be saved, can be a DataFrame, dictionary, list of lists, or a NumPy array.\n",
    "    base_filename (str): Base filename without extension.\n",
    "    \"\"\"\n",
    "    # Convert the input data to a DataFrame if it's not already one\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Construct filename with date, time, and .csv extension\n",
    "    filename = f\"{base_filename}_{current_time}.csv\"\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "def save_submission_csv(test_data, predictions, model_name):\n",
    "    \"\"\"\n",
    "    Save model predictions to a CSV file with a formatted filename.\n",
    "\n",
    "    Args:\n",
    "        test_data (pd.DataFrame): The test data.\n",
    "        predictions (pd.Series or np.array): Model predictions for the test data.\n",
    "        model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a dictionary for storing results with Ids and predictions\n",
    "    results_dict = {'Id': np.arange(test_data.shape[0]), 'log_bike_count': predictions}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "    # Format the submission CSV filename with model name, date, and time\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    submission_filename = f\"submission_{model_name}_{current_datetime}.csv\"\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(submission_filename, index=False)\n",
    "    print(results_df)\n",
    "    \n",
    "def preprocess_data(train_file, test_file, final_test_file, weather_file, \n",
    "                    lockdown_file, holiday_file, subscribers_file, sncf_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the bike sharing data by combining, cleaning, and merging it with \n",
    "    external data sources like weather, holidays, and lockdown information.\n",
    "\n",
    "    Parameters:\n",
    "    - train_file: Path to the training data file.\n",
    "    - test_file: Path to the test data file.\n",
    "    - final_test_file: Path to the final test data file.\n",
    "    - weather_file: Path to the weather data file.\n",
    "    - lockdown_file: Path to the lockdown data file.\n",
    "    - holiday_file: Path to the holiday data file.\n",
    "    - subscribers_file: Path to the subscribers data file.\n",
    "    - sncf_file: Path to the SNCF (French National Railway Company) data file.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: Preprocessed training data.\n",
    "    - test_data: Preprocessed test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load main datasets\n",
    "    train_data = pd.read_parquet(train_file)\n",
    "    test_data = pd.read_parquet(test_file)\n",
    "    final_test_data = pd.read_parquet(final_test_file)\n",
    "\n",
    "    # Combining training and test datasets\n",
    "    combined_train_test = pd.concat([train_data, test_data], axis=0)\n",
    "    combined_train_test.dropna(inplace=True)\n",
    "    train_data = combined_train_test\n",
    "    test_data = final_test_data\n",
    "\n",
    "    # Load and preprocess external datasets\n",
    "    weather_data = pd.read_csv(weather_file)\n",
    "    lockdown_data = pd.read_csv(lockdown_file)\n",
    "    holiday_data = pd.read_csv(holiday_file)\n",
    "    velib_subscribers = pd.read_csv(subscribers_file)\n",
    "    sncf_passengers_delayed = pd.read_csv(sncf_file)\n",
    "\n",
    "    # Standardize date columns across all datasets\n",
    "    standardize_date_column(train_data, test_data, final_test_data, weather_data, \n",
    "                            lockdown_data, holiday_data, velib_subscribers, sncf_passengers_delayed)\n",
    "\n",
    "    # Select relevant columns from weather and lockdown data\n",
    "    weather_data = weather_data[['date', 'feelslike', 'humidity', 'precip', 'windspeed']]\n",
    "    lockdown_data = lockdown_data[['date', 'school_closures', 'full_lockdown']]\n",
    "\n",
    "    # Merge external data with main datasets\n",
    "    external_datasets = [holiday_data, weather_data, lockdown_data, \n",
    "                         velib_subscribers, sncf_passengers_delayed]\n",
    "    train_data = merge_all_external_data(train_data, external_datasets)\n",
    "    test_data = merge_all_external_data(test_data, external_datasets)\n",
    "\n",
    "    # Apply transformations: date encoding and temperature binning\n",
    "    train_data = _encode_dates(train_data)\n",
    "    train_data = bin_temperature(train_data)\n",
    "    test_data = _encode_dates(test_data)\n",
    "    test_data = bin_temperature(test_data)\n",
    "\n",
    "    # Remove outliers from training data\n",
    "    train_data = remove_outliers(train_data, 'log_bike_count')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['counter_id', 'counter_installation_date', 'counter_technical_id', \n",
    "                       'coordinates', 'site_id', 'site_name', 'latitude', 'longitude', 'bike_count']\n",
    "    train_data.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    test_data.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "    # Ensure test data has the same feature set as training data, except the target variable\n",
    "    for column in train_data.columns:\n",
    "        if column not in test_data.columns and column != 'log_bike_count':\n",
    "            test_data[column] = 0\n",
    "\n",
    "    # Align columns in test data to match training data\n",
    "    test_data = test_data[train_data.columns.drop('log_bike_count')] \n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def standardize_date_column(*dataframes):\n",
    "    \"\"\"\n",
    "    Standardizes the date column across multiple dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes: A variable number of pandas dataframes.\n",
    "\n",
    "    For each dataframe, this function renames 'datetime' column to 'date' (if present)\n",
    "    and converts 'date' column to a pandas datetime object.\n",
    "    \"\"\"\n",
    "    for df in dataframes:\n",
    "        # Rename 'datetime' column to 'date' if it exists\n",
    "        if 'datetime' in df.columns:\n",
    "            df.rename(columns={'datetime': 'date'}, inplace=True)\n",
    "        \n",
    "        # Convert 'date' column to datetime object\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        \n",
    "def merge_all_external_data(main_data, external_datasets):\n",
    "    \"\"\"\n",
    "    Merges the main dataset with a list of external datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - main_data: The primary pandas dataframe.\n",
    "    - external_datasets: A list of pandas dataframes to be merged with the main dataframe.\n",
    "\n",
    "    The function merges each external dataset with the main dataset on the 'date' column\n",
    "    and removes any duplicate 'date' columns post-merge.\n",
    "    \"\"\"\n",
    "    for dataset in external_datasets:\n",
    "        # Merge main_data with an external dataset\n",
    "        main_data = _merge_external_data(main_data, dataset, 'date')\n",
    "        \n",
    "        # Drop duplicate 'date' column if created during the merge\n",
    "        main_data.drop(columns=['date_y'], inplace=True, errors='ignore')\n",
    "    \n",
    "    return main_data\n",
    "\n",
    "\n",
    "def _encode_dates(X):\n",
    "    \"\"\"\n",
    "    Encodes date-related features in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - X: The pandas dataframe with a 'date' column.\n",
    "\n",
    "    The function extracts various date components, checks for holidays, and creates\n",
    "    cyclical features for time-based attributes. It also one-hot encodes some of the\n",
    "    date components and then drops the original date-related columns.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    fr_holidays = holidays.France()  # Get the holiday calendar for France\n",
    "\n",
    "    # Extract date components\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "    X[\"week\"] = X[\"date\"].dt.isocalendar().week\n",
    "\n",
    "    # Determine if the date is a French holiday\n",
    "    X[\"is_holiday\"] = X[\"date\"].apply(lambda d: d in fr_holidays).astype(int)\n",
    "\n",
    "    # Cosine and sine encodings for hours, months, and weekdays\n",
    "    X[\"hour_sin\"] = np.sin(2 * np.pi * X[\"hour\"] / 23.0)\n",
    "    X[\"hour_cos\"] = np.cos(2 * np.pi * X[\"hour\"] / 23.0)\n",
    "    X[\"weekday_sin\"] = np.sin(2 * np.pi * X[\"weekday\"] / 6.0)\n",
    "    X[\"weekday_cos\"] = np.cos(2 * np.pi * X[\"weekday\"] / 6.0)\n",
    "    X[\"month_sin\"] = np.sin(2 * np.pi * X[\"month\"] / 11.0)\n",
    "    X[\"month_cos\"] = np.cos(2 * np.pi * X[\"month\"] / 11.0)\n",
    "\n",
    "    # Season encoding\n",
    "    X[\"season\"] = X[\"month\"].apply(lambda m: (m % 12 + 3) // 3)\n",
    "    # Encode seasons as sine and cosine\n",
    "    X[\"season_cos\"] = np.cos(2 * np.pi * X[\"season\"] / 3.0)\n",
    "    X[\"season_sin\"] = np.sin(2 * np.pi * X[\"season\"] / 3.0)\n",
    "\n",
    "    # Rush hour for weekdays not on holidays\n",
    "    X[\"morning_rush\"] = ((X[\"weekday\"] < 5) & (X[\"hour\"] >= 7) & (X[\"hour\"] <= 9) & (X[\"is_holiday\"] == 0)).astype(int)\n",
    "    X[\"evening_rush\"] = ((X[\"weekday\"] < 5) & (X[\"hour\"] >= 16) & (X[\"hour\"] <= 18) & (X[\"is_holiday\"] == 0)).astype(int)\n",
    "\n",
    "    # One-hot encode year and weekday\n",
    "    year_dummies = pd.get_dummies(X['year'], prefix='year')\n",
    "\n",
    "    # Concatenate with original DataFrame\n",
    "    X = pd.concat([X, year_dummies], axis=1)\n",
    "\n",
    "    # Drop original date components\n",
    "    X.drop(columns=['year', 'month', 'day', 'weekday', 'hour', 'week', 'date','season'], inplace=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def bin_temperature(df):\n",
    "    \"\"\"\n",
    "    Bins temperature values and one-hot encodes the binned categories.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The pandas dataframe with a 'feelslike' column representing temperature.\n",
    "\n",
    "    The function bins the temperature into categories ('cold', 'cool', 'warm', 'hot')\n",
    "    and then one-hot encodes these categories. It removes the original temperature column\n",
    "    after binning.\n",
    "    \"\"\"\n",
    "    bins = [-float('inf'), 10, 20, 25, float('inf')]\n",
    "    labels = ['cold', 'cool', 'warm', 'hot']\n",
    "    df['temp_binned'] = pd.cut(df['feelslike'], bins=bins, labels=labels)\n",
    "\n",
    "    # One-hot encode the binned temperatures\n",
    "    temp_dummies = pd.get_dummies(df['temp_binned'], prefix='temp')\n",
    "    df = pd.concat([df, temp_dummies], axis=1)\n",
    "\n",
    "    # Drop original binned column and 'feelslike'\n",
    "    df.drop(columns=['temp_binned'], inplace=True)\n",
    "    return df\n",
    "\n",
    "        \n",
    "def _merge_external_data(X, df_ext, on_column):\n",
    "    \"\"\"\n",
    "    Merges two dataframes on a specified column using an \"asof\" merge.\n",
    "\n",
    "    Parameters:\n",
    "    - X: The primary pandas dataframe.\n",
    "    - df_ext: The external pandas dataframe to merge.\n",
    "    - on_column: The column name to merge on.\n",
    "\n",
    "    Returns a merged dataframe while preserving the original order of the primary dataframe.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X[\"orig_index\"] = np.arange(X.shape[0])\n",
    "    \n",
    "    # Perform an \"asof\" merge, which is useful for time-series data\n",
    "    X = pd.merge_asof(X.sort_values('date'), df_ext.sort_values('date'), on=on_column)\n",
    "    \n",
    "    # Restore the original order\n",
    "    X = X.sort_values(\"orig_index\")\n",
    "    del X[\"orig_index\"]\n",
    "    \n",
    "    return X\n",
    "    \n",
    "\n",
    "def remove_outliers(df, column, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers from a dataframe with Poisson-distributed data by applying\n",
    "    a square root transformation and using the IQR method.\n",
    "\n",
    "    :param df: DataFrame to process.\n",
    "    :param column: The name of the column to check for outliers.\n",
    "    :param multiplier: The multiplier for the IQR to define what is considered an outlier.\n",
    "    :return: DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    # Apply square root transformation\n",
    "    transformed_col = np.sqrt(df[column])\n",
    "\n",
    "    # Compute IQR on the transformed data\n",
    "    Q1 = transformed_col.quantile(0.10)\n",
    "    Q3 = transformed_col.quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calculate bounds\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "    # Identify outliers and filter them out\n",
    "    outlier_mask = (transformed_col >= lower_bound) & (transformed_col <= upper_bound)\n",
    "    return df[outlier_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99545069",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed, test_data_processed = preprocess_data(\n",
    "    \"train.parquet\", \"test.parquet\", \"final_test.parquet\", \n",
    "    \"hourly-weather-data.csv\", \"lockdown-data.csv\", \"paris_school_holidays_2020_2022_correct.csv\",\n",
    "    \"velib_subscribers_2020_2022.csv\", \"sncf_passengers_delayed_hourly_2020_2022.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fffb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the X y from the training data \n",
    "X = train_data_processed.drop('log_bike_count', axis=1)\n",
    "y = train_data_processed['log_bike_count']\n",
    "X_test=test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783db4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_name is the only categorical column\n",
    "categorical_features = ['counter_name']\n",
    "numerical_features = [\"feelslike\", \"humidity\", \"precip\",\"windspeed\",\"Subscribers\",\"SNCFpassengersDelayedInParisPerHour\"]\n",
    "\n",
    "# Preprocessor for XGBoost\n",
    "xgb_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through other features without transformation\n",
    ")\n",
    "\n",
    "# Preprocessor for CatBoost\n",
    "catboost_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Passes through other features including the categorical feature without transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29cb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost with the best parameters\n",
    "xgb_params = {\n",
    "    'colsample_bytree': 0.9770938171127415,\n",
    "    'gamma': 0.47218960940491295,\n",
    "    'learning_rate': 0.11795427856021579,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 4.0,\n",
    "    'n_estimators': 600,\n",
    "    'reg_alpha': 0.031467061215090325,\n",
    "    'reg_lambda': 0.03673201979521254,\n",
    "    'subsample': 0.68118920119262,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Create the XGBoost model with the specified parameters\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Create the XGBoost pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', xgb_preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31924be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'Subscribers' column in the training set\n",
    "X['Subscribers'] = X['Subscribers'] / 400000  # This because the preprocessor changes the indices for Catboost causing problems\n",
    "\n",
    "# Scale the 'Subscribers' column in the test set\n",
    "X_test['Subscribers'] = X_test['Subscribers'] / 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8d788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index of the categorical feature\n",
    "cat_features_index = [X.columns.get_loc('counter_name')]\n",
    "\n",
    "# CatBoost best parameters \n",
    "catboost_params = {\n",
    "    'depth': 12,\n",
    "    'iterations': 1500,\n",
    "    'l2_leaf_reg': 3.87741648,\n",
    "    'rsm': 0.495940279,\n",
    "    'subsample': 0.445083553,\n",
    "    'cat_features': cat_features_index,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "# Initialize CatBoost with the best parameters\n",
    "catboost_model = CatBoostRegressor(**catboost_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083bf44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the models\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgboost', xgb_pipeline),  \n",
    "        ('catboost', catboost_model)\n",
    "    ],\n",
    "    final_estimator=LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f324059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.6212873\ttotal: 369ms\tremaining: 9m 13s\n",
      "100:\tlearn: 0.6349083\ttotal: 18.2s\tremaining: 4m 12s\n",
      "200:\tlearn: 0.5306127\ttotal: 36.2s\tremaining: 3m 53s\n",
      "300:\tlearn: 0.4944144\ttotal: 54.9s\tremaining: 3m 38s\n",
      "400:\tlearn: 0.4709447\ttotal: 1m 14s\tremaining: 3m 22s\n",
      "500:\tlearn: 0.4562403\ttotal: 1m 34s\tremaining: 3m 9s\n",
      "600:\tlearn: 0.4450752\ttotal: 2m\tremaining: 3m\n",
      "700:\tlearn: 0.4342030\ttotal: 2m 23s\tremaining: 2m 43s\n",
      "800:\tlearn: 0.4268289\ttotal: 2m 44s\tremaining: 2m 23s\n",
      "900:\tlearn: 0.4204414\ttotal: 3m 5s\tremaining: 2m 3s\n",
      "1000:\tlearn: 0.4152995\ttotal: 3m 28s\tremaining: 1m 43s\n",
      "1100:\tlearn: 0.4099459\ttotal: 3m 50s\tremaining: 1m 23s\n",
      "1200:\tlearn: 0.4058803\ttotal: 4m 12s\tremaining: 1m 2s\n",
      "1300:\tlearn: 0.4018360\ttotal: 4m 35s\tremaining: 42.1s\n",
      "1400:\tlearn: 0.3980336\ttotal: 4m 57s\tremaining: 21s\n",
      "1499:\tlearn: 0.3944644\ttotal: 5m 19s\tremaining: 0us\n",
      "0:\tlearn: 1.6191745\ttotal: 188ms\tremaining: 4m 42s\n",
      "100:\tlearn: 0.6432788\ttotal: 19.2s\tremaining: 4m 25s\n",
      "200:\tlearn: 0.5329673\ttotal: 39.5s\tremaining: 4m 14s\n",
      "300:\tlearn: 0.4940063\ttotal: 59.9s\tremaining: 3m 58s\n",
      "400:\tlearn: 0.4692526\ttotal: 1m 20s\tremaining: 3m 40s\n",
      "500:\tlearn: 0.4511926\ttotal: 1m 41s\tremaining: 3m 22s\n",
      "600:\tlearn: 0.4384120\ttotal: 2m 2s\tremaining: 3m 3s\n",
      "700:\tlearn: 0.4292156\ttotal: 2m 23s\tremaining: 2m 44s\n",
      "800:\tlearn: 0.4206346\ttotal: 2m 46s\tremaining: 2m 25s\n",
      "900:\tlearn: 0.4145205\ttotal: 3m 9s\tremaining: 2m 6s\n",
      "1000:\tlearn: 0.4087648\ttotal: 3m 32s\tremaining: 1m 46s\n",
      "1100:\tlearn: 0.4037380\ttotal: 3m 57s\tremaining: 1m 25s\n",
      "1200:\tlearn: 0.3991974\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "1300:\tlearn: 0.3951120\ttotal: 4m 51s\tremaining: 44.5s\n",
      "1400:\tlearn: 0.3911762\ttotal: 5m 18s\tremaining: 22.5s\n",
      "1499:\tlearn: 0.3874786\ttotal: 5m 44s\tremaining: 0us\n",
      "0:\tlearn: 1.6444303\ttotal: 215ms\tremaining: 5m 22s\n",
      "100:\tlearn: 0.6281237\ttotal: 19.7s\tremaining: 4m 32s\n",
      "200:\tlearn: 0.5150092\ttotal: 40s\tremaining: 4m 18s\n",
      "300:\tlearn: 0.4798512\ttotal: 1m 1s\tremaining: 4m 3s\n",
      "400:\tlearn: 0.4584155\ttotal: 1m 25s\tremaining: 3m 54s\n",
      "500:\tlearn: 0.4425191\ttotal: 1m 50s\tremaining: 3m 39s\n",
      "600:\tlearn: 0.4313595\ttotal: 2m 13s\tremaining: 3m 20s\n",
      "700:\tlearn: 0.4223478\ttotal: 2m 36s\tremaining: 2m 58s\n",
      "800:\tlearn: 0.4153075\ttotal: 3m 1s\tremaining: 2m 38s\n",
      "900:\tlearn: 0.4091897\ttotal: 3m 27s\tremaining: 2m 18s\n",
      "1000:\tlearn: 0.4037773\ttotal: 4m 7s\tremaining: 2m 3s\n",
      "1100:\tlearn: 0.3991830\ttotal: 4m 37s\tremaining: 1m 40s\n",
      "1200:\tlearn: 0.3951257\ttotal: 5m 3s\tremaining: 1m 15s\n",
      "1300:\tlearn: 0.3904702\ttotal: 5m 33s\tremaining: 51s\n",
      "1400:\tlearn: 0.3867840\ttotal: 6m 6s\tremaining: 25.9s\n",
      "1499:\tlearn: 0.3834469\ttotal: 6m 36s\tremaining: 0us\n",
      "0:\tlearn: 1.6479006\ttotal: 216ms\tremaining: 5m 23s\n",
      "100:\tlearn: 0.6481928\ttotal: 20.6s\tremaining: 4m 45s\n",
      "200:\tlearn: 0.5385198\ttotal: 42.3s\tremaining: 4m 33s\n",
      "300:\tlearn: 0.5014845\ttotal: 1m 4s\tremaining: 4m 16s\n",
      "400:\tlearn: 0.4731636\ttotal: 1m 26s\tremaining: 3m 56s\n",
      "500:\tlearn: 0.4559797\ttotal: 1m 48s\tremaining: 3m 35s\n",
      "600:\tlearn: 0.4431071\ttotal: 2m 11s\tremaining: 3m 16s\n",
      "700:\tlearn: 0.4325266\ttotal: 2m 39s\tremaining: 3m 2s\n",
      "800:\tlearn: 0.4242466\ttotal: 3m 4s\tremaining: 2m 41s\n",
      "900:\tlearn: 0.4173595\ttotal: 3m 29s\tremaining: 2m 19s\n",
      "1000:\tlearn: 0.4119378\ttotal: 3m 55s\tremaining: 1m 57s\n",
      "1100:\tlearn: 0.4066791\ttotal: 4m 21s\tremaining: 1m 34s\n",
      "1200:\tlearn: 0.4022838\ttotal: 4m 47s\tremaining: 1m 11s\n",
      "1300:\tlearn: 0.3979167\ttotal: 5m 14s\tremaining: 48.1s\n",
      "1400:\tlearn: 0.3933346\ttotal: 5m 44s\tremaining: 24.3s\n",
      "1499:\tlearn: 0.3898350\ttotal: 6m 15s\tremaining: 0us\n",
      "0:\tlearn: 1.6101951\ttotal: 246ms\tremaining: 6m 9s\n",
      "100:\tlearn: 0.6441768\ttotal: 20.7s\tremaining: 4m 47s\n",
      "200:\tlearn: 0.5329201\ttotal: 42s\tremaining: 4m 31s\n",
      "300:\tlearn: 0.4917379\ttotal: 1m 3s\tremaining: 4m 13s\n",
      "400:\tlearn: 0.4675927\ttotal: 1m 25s\tremaining: 3m 55s\n",
      "500:\tlearn: 0.4502632\ttotal: 1m 48s\tremaining: 3m 35s\n",
      "600:\tlearn: 0.4380153\ttotal: 2m 11s\tremaining: 3m 15s\n",
      "700:\tlearn: 0.4286588\ttotal: 2m 34s\tremaining: 2m 55s\n",
      "800:\tlearn: 0.4206591\ttotal: 2m 58s\tremaining: 2m 35s\n",
      "900:\tlearn: 0.4140992\ttotal: 3m 22s\tremaining: 2m 14s\n",
      "1000:\tlearn: 0.4082835\ttotal: 3m 47s\tremaining: 1m 53s\n",
      "1100:\tlearn: 0.4031947\ttotal: 4m 13s\tremaining: 1m 31s\n",
      "1200:\tlearn: 0.3981359\ttotal: 4m 39s\tremaining: 1m 9s\n",
      "1300:\tlearn: 0.3941881\ttotal: 5m 7s\tremaining: 47s\n",
      "1400:\tlearn: 0.3904722\ttotal: 5m 35s\tremaining: 23.7s\n",
      "1499:\tlearn: 0.3869533\ttotal: 6m 4s\tremaining: 0us\n",
      "0:\tlearn: 1.5850194\ttotal: 268ms\tremaining: 6m 41s\n",
      "100:\tlearn: 0.5648278\ttotal: 20s\tremaining: 4m 37s\n",
      "200:\tlearn: 0.4973931\ttotal: 41.1s\tremaining: 4m 25s\n",
      "300:\tlearn: 0.4651818\ttotal: 1m 4s\tremaining: 4m 16s\n",
      "400:\tlearn: 0.4442941\ttotal: 1m 32s\tremaining: 4m 12s\n",
      "500:\tlearn: 0.4305653\ttotal: 2m 2s\tremaining: 4m 4s\n",
      "600:\tlearn: 0.4202881\ttotal: 2m 30s\tremaining: 3m 45s\n",
      "700:\tlearn: 0.4109601\ttotal: 3m 1s\tremaining: 3m 26s\n",
      "800:\tlearn: 0.4036885\ttotal: 3m 30s\tremaining: 3m 3s\n",
      "900:\tlearn: 0.3974694\ttotal: 4m\tremaining: 2m 39s\n",
      "1000:\tlearn: 0.3927047\ttotal: 4m 26s\tremaining: 2m 12s\n",
      "1100:\tlearn: 0.3886148\ttotal: 4m 55s\tremaining: 1m 47s\n",
      "1200:\tlearn: 0.3851196\ttotal: 5m 25s\tremaining: 1m 21s\n",
      "1300:\tlearn: 0.3818307\ttotal: 5m 54s\tremaining: 54.3s\n",
      "1400:\tlearn: 0.3786894\ttotal: 6m 24s\tremaining: 27.2s\n",
      "1499:\tlearn: 0.3750721\ttotal: 6m 54s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;xgboost&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                                  transformers=[(&#x27;num&#x27;,\n",
       "                                                                                 StandardScaler(),\n",
       "                                                                                 [&#x27;feelslike&#x27;,\n",
       "                                                                                  &#x27;humidity&#x27;,\n",
       "                                                                                  &#x27;precip&#x27;,\n",
       "                                                                                  &#x27;windspeed&#x27;,\n",
       "                                                                                  &#x27;Subscribers&#x27;,\n",
       "                                                                                  &#x27;SNCFpassengersDelayedInParisPerHour&#x27;]),\n",
       "                                                                                (&#x27;cat&#x27;,\n",
       "                                                                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                                 [&#x27;counter_name&#x27;])])),\n",
       "                                               (&#x27;model&#x27;,\n",
       "                                                XGBRegressor(base_scor...\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=10,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=4.0,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=600,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=None, ...))])),\n",
       "                              (&#x27;catboost&#x27;,\n",
       "                               &lt;catboost.core.CatBoostRegressor object at 0x0000025F2CA40130&gt;)],\n",
       "                  final_estimator=LinearRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;xgboost&#x27;,\n",
       "                               Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                                  transformers=[(&#x27;num&#x27;,\n",
       "                                                                                 StandardScaler(),\n",
       "                                                                                 [&#x27;feelslike&#x27;,\n",
       "                                                                                  &#x27;humidity&#x27;,\n",
       "                                                                                  &#x27;precip&#x27;,\n",
       "                                                                                  &#x27;windspeed&#x27;,\n",
       "                                                                                  &#x27;Subscribers&#x27;,\n",
       "                                                                                  &#x27;SNCFpassengersDelayedInParisPerHour&#x27;]),\n",
       "                                                                                (&#x27;cat&#x27;,\n",
       "                                                                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                                 [&#x27;counter_name&#x27;])])),\n",
       "                                               (&#x27;model&#x27;,\n",
       "                                                XGBRegressor(base_scor...\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=10,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=4.0,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=600,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=None, ...))])),\n",
       "                              (&#x27;catboost&#x27;,\n",
       "                               &lt;catboost.core.CatBoostRegressor object at 0x0000025F2CA40130&gt;)],\n",
       "                  final_estimator=LinearRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgboost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                 [&#x27;feelslike&#x27;, &#x27;humidity&#x27;, &#x27;precip&#x27;,\n",
       "                                  &#x27;windspeed&#x27;, &#x27;Subscribers&#x27;,\n",
       "                                  &#x27;SNCFpassengersDelayedInParisPerHour&#x27;]),\n",
       "                                (&#x27;cat&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;counter_name&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;feelslike&#x27;, &#x27;humidity&#x27;, &#x27;precip&#x27;, &#x27;windspeed&#x27;, &#x27;Subscribers&#x27;, &#x27;SNCFpassengersDelayedInParisPerHour&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;counter_name&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.9770938171127415, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0.47218960940491295,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.11795427856021579,\n",
       "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=4.0, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=600, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>catboost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x0000025F2CA40130&gt;</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('xgboost',\n",
       "                               Pipeline(steps=[('preprocessor',\n",
       "                                                ColumnTransformer(remainder='passthrough',\n",
       "                                                                  transformers=[('num',\n",
       "                                                                                 StandardScaler(),\n",
       "                                                                                 ['feelslike',\n",
       "                                                                                  'humidity',\n",
       "                                                                                  'precip',\n",
       "                                                                                  'windspeed',\n",
       "                                                                                  'Subscribers',\n",
       "                                                                                  'SNCFpassengersDelayedInParisPerHour']),\n",
       "                                                                                ('cat',\n",
       "                                                                                 OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                                 ['counter_name'])])),\n",
       "                                               ('model',\n",
       "                                                XGBRegressor(base_scor...\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=10,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=4.0,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=600,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=None, ...))])),\n",
       "                              ('catboost',\n",
       "                               <catboost.core.CatBoostRegressor object at 0x0000025F2CA40130>)],\n",
       "                  final_estimator=LinearRegression())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the training data\n",
    "stacked_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc152b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = stacked_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2895ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  log_bike_count\n",
      "0          0        0.418518\n",
      "1          1        1.272843\n",
      "2          2        2.127003\n",
      "3          3        0.641420\n",
      "4          4        0.530446\n",
      "...      ...             ...\n",
      "51435  51435        4.440238\n",
      "51436  51436        4.923691\n",
      "51437  51437        5.167116\n",
      "51438  51438        4.564830\n",
      "51439  51439        3.895252\n",
      "\n",
      "[51440 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save the submission into a .csv file\n",
    "save_submission_csv(X_test, predictions,\"Submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
